% compile using latex (xelatex seems to crop the figure strangley)
\documentclass{article} % For LaTeX2e
\usepackage[dvips]{graphicx}
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
%\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{rotating}
\usepackage[outdir=./]{epstopdf}

\usepackage{graphics,graphicx}
\usepackage{amsmath,amssymb,amsfonts,comment}
\definecolor{LinkColor}{rgb}{0,0,0}    

\newcommand{\p}{\text{P}}
\newcommand{\word}[1]{\texttt{#1}}
\urldef{\googlegroup}\url{https://groups.google.com/forum/#\!forum/word2vec-toolkit}
\urldef{\blogpost}\url{https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/}

\title{Corpus Experiments for Word Embeddings}

 \author{
 	Benjamin Wilson\\
	Lateral GmbH\\
	\texttt{benjamin@lateral.io}
	\And
	Adriaan Schakel\\
	\texttt{adriaan.schakel@gmail.com}
 }

\date{\today}
\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\graphicspath{{../outputs/}}
\maketitle


\begin{abstract}
	We introduce an experimental approach to studying the properties of word embeddings.
	Controlled experiments, acheived by modification of the training corpus, permit the demonstration of direct, causal relationships between word properties and word vector direction and length.
	This approach is demonstrated in the case of the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise.
\end{abstract} 

\begin{section}{Introduction}
Word embeddings are typically trained with samples from the word co-occurrence distributions.
Recall that the co-occurrence distribution of a word $w$ gives the probability $\p(w'|w)$ that a word $w'$ occurs nearby, given that $w$ occurred.
Samples of the co-occurrence distributions are obtained by scanning a short window over the text.
The non-central (or ``context'') words are treated as samples of the co-occurrence distribution the middle (or ``current'') word. 
In this way, the word vectors of the word embedding are determined by the co-occurrence distributions and frequencies of the words in the vocabulary.

In this short note, we perform experiments that illustrate the effect of these two determining pieces of information (co-occurrence distribution and word frequency) as they are varied independently of one another.
This is acheived through the introduction of new ``words'' into the training corpus with varying frequencies and varying levels of noise in their co-occurrence distributions.
The frequency and co-occurrence distributions of these introduced words are modelled on existing words in the corpus.

We illustrate our approach in case of the word2vec CBOW model.
However, these experiments (and other besides) could might equally well be performed for word embedding methods such as word2vec skip-gram \cite{DistRepns,EfficientEstimation}, GloVe \cite{pennington2014glove} and SENNA \cite{collobert-2011}.
In the case of word2vec, we show that word vector length depends directly and linearly with word frequency.
We show furthermore that word vector length depends directly and linearly with the level of noise in the co-occurrence distribution of the word.
In both cases, the coefficient of linearity depends upon the word.

In the word2vec similarity and word relationship tasks, normalised word vectors are used.
Word vector length is thus disregarded.
In the word2vec forum\footnote{\googlegroup}, it has been observed that normalisation improves performance on theses tasks since word vector length is related to word frequency.
Figure \ref{fig:frequency-norm-graph} illustrates the global relationship between vector length and word frequency.
It remained unclear, however, whether word frequency was \textit{directly} related to word vector length or rather via correlated factors such as word significance.
Our experimental approach demonstrates that a direct relationship indeed exists.
It is shown moreover that the normalisation of the word vectors also prevents word co-occurrence noise from affecting the performance of the word similarity and word relationship tasks.

\begin{subsection}{Previous work}
Our experimental finding, in the context of word2vec, that word vector length decreases with co-occurrence noise is reminiscent of \cite{vecchi-baroni-zamparelli2011}, where a relationship between vector length and the ``semantic deviance'' of an adjective-noun composition was studied observationally.
In \cite{schakel-wilson}, the interpretation of word vector length is an indication of word significance is explored in the context of the arXiv high-energy physics corpus.
\end{subsection}
 
\begin{figure}
	\includegraphics[scale=0.3]{frequency-norm-scatterplot}
	\caption{ The global relationship between word frequency and vector length.  }
	\label{fig:frequency-norm-graph}
\end{figure}
\end{section}

\begin{section}{Corpus and model}
Our training data is built from the Wikipedia XML datadump from October 2013.
In order to remove from the training data the bulk of robot-generated pages on Wikipedia, only pages with at least 20 monthly page views are retained\footnote{For further justification of this rationale, and to obtain the dataset, see \blogpost}.
Stubs and disambiguation pages are also removed, leaving 463 thousand pages with a total of 482 million words.

This base corpus is then modified as described in section \ref{experimental_design}.
Punctuation and numbers were removed and the corpus lower-cased prior to the modifications.
For recognisability, the tokens introduced into the corpus during the modification are uppercased.

For simplicity, only the CBOW method with a single set of hyperparameters is considered.
Specifically, a $100$-dimensional CBOW model is trained using negative sampling with 5 negative samples, a window size of $10$, a minimum word occurrence of $200$, and $10$ passes through the corpus.
Subsampling was not used so that influence of word frequency could be more clearly discerned.
Identical experimental results were obtained using hierarchical softmax, but these are omitted for succinctness.
The relatively high minimum word cut-off is chosen to ensure that word vectors receive a sufficient number of gradient updates to be meaningful.

The most recent revision of word2vec was used\footnote{SVN revision 42, see \url{http://word2vec.googlecode.com/svn/trunk/}}.
The source code for performing the experiments is made available on GitHub\footnote{\url{https://github.com/benjaminwilson/word2vec-norm-experiments}}.

\begin{figure}\label{fig:frequency-histogram}
	\includegraphics[scale=0.5]{occurrence-histogram}
	\caption{
	Word frequency histogram.
	Here and elsewhere, frequency bands are delimited by powers of $2$.
	}
\end{figure}

\begin{subsection}{Replacement procedure}\label{replacement-procedure}
In the experiments, tokens are introduced into corpus via a replacement procedure.
To illustrate this procedure, consider a word, e.g. \word{cat}.
For each occurrence of this word, a sample $i$, $1 \leqslant i \leqslant n$ is drawn from a truncated geometric distribution, and that occurrence of \word{cat} is replaced with \word{CAT\_i}.
Thus the word \word{cat} is replaced throughout the corpus by a family of tokens with varying frequencies but approximately the same co-occurrence distribution as \word{cat}.

The geometric distribution is truncated in order to limit the number of tokens introduced into the corpus.
For any ratio $0 < \lambda < 1$ and maximum value $n > 0$, the truncated geometric distribution is given by
$$ P_{\lambda, n} (i) = \frac{\lambda^{i-1} \cdot (1-\lambda)}{(1 - \lambda^n)}, \qquad 1 \leqslant i \leqslant n.$$ 
This is the distribution over $i = 1, 2, \dots n$ for which the probabilities decay exponentially base $\lambda$.
Of course, other distributions might equally have been chosen for the experiments.
\end{subsection}
\end{section}

\begin{section}{Word frequency variation experiment}\label{WFVE}
In this experiment, families of tokens are introduced into the corpus.
The tokens in each family vary in frequency but share a common co-occurrence distribution.
After model training, comparison of the resulting word vectors for these tokens reveals the independent effect of frequency variation on the word embedding.

\begin{subsection}{Tokens derived from existing words}
A small set of words from the unmodified corpus is chosen uniformly at random from the vocabulary.
In order that the introduced tokens do not have too low a frequency, only words which occur at least 10 thousand times are chosen.
To this set of words we add the word \word{the}, in order to include a high-frequency stopword.
The replacement procedure of subsection \ref{replacement-procedure} is then performed for each of these words, using a geometric rate of decay of $\lambda = 0.5$, and maximum value $n=20$.
This value of $\lambda$ is one of a range of values that ensure that, for each word, multiple tokens will be introduced with a number of occurrences sufficient to survive the chosen word2vec minimum occurrence cut-off of 200.  
A maximum value $n=20$ suffices for this choice of $\lambda$, since $2^{20 + \log_2{200}}$ exceeds the maximum occurrence of any token in the corpus. 
Figure \ref{fig:word-frequency-experiment-text-cat} illustrates the effect of these modifications on a sample text, with a family of tokens \word{CAT\_i}, derived from the word \word{cat}.

Figure \ref{fig:word-frequency-counts} gives the number of occurrences of the words chosen for this experiment.

\begin{figure}\label{fig:word-frequency-counts}
	\input{word-frequency-experiment-counts.tex}
	\caption{Occurrence counts for words chosen for the word frequency experiment. }
\end{figure}

\begin{figure}
	\input{word-frequency-experiment-text-cat.tex}
	\caption{Text modified for the word frequency experiment, where the word \word{cat} was chosen, $\lambda=0.5$ and $n=20$.}
	\label{fig:word-frequency-experiment-text-cat}
\end{figure}

\end{subsection}

\begin{subsection}{Tokens derived from an introduced, meaningless word}
	A high-frequency, purely meaningless word is included in the experiment for comparison.
	We choose to introduce a new, entirely meaningless token \word{VOID} into the corpus, rather than choose an existing word whose meaninglessness is only supposed.
	To achieve this, the token is interspersed uniformly at random throughout the corpus so that its global frequency is $0.005$.
	The co-occurrence distribution of this introduced token thus coincides with the global frequency distribution.
	The replacement procedure is then performed for \word{VOID}, using the same values for $\lambda$ and $n$ as above.
Figure \ref{fig:word-frequency-experiment-text-void} illustrates the effect of this modifications on a sample text.

\begin{figure}\label{fig:word-frequency-experiment-text-void}
	\input{word-frequency-experiment-text-void.tex}
	\caption{Text modified for the word frequency experiment, where the
	word \word{cat} was chosen, $\lambda=0.5$ and $n=20$.}
\end{figure}
\end{subsection}

\subsection{Experimental results}
Word frequency has no effect on word vector direction.
Figure \ref{word-frequency-experiment-heatmap} shows the cosine similarity of the pairs of word vectors for the tokens introduced into the corpus.
The cosine similarity measures the extent to which two vectors have the same direction, taking a maximum value of $1$ and a minimum value of $0$.
Notice that the word vectors associated to tokens derived from meaningful tokens have the same direction.
The same is true of the word vectors associated to the \word{VOID\_i} for $i$ \textcolor{red}{below $7$ or $8$}, but that the word vectors \word{VOID\_i} gradually diverge directionally for higher values of $i$.
This demonstrates the difficulty of learning the co-occurrence distribution of $\word{VOID}$, the global frequency distribution.
Because of its noisiness, a higher number of samples (occurrences) is required to learn this co-occurrence distribution.

It remains, therefore, to consider the effect of frequency variation on word vector length.
Figure \ref{fig:frequency-norm-graph} illustrates the global relationship between vector length and word frequency.
Figure \ref{fig:word-frequency-experiment-graph}, on the other hand, shows this relationship for individual words.
Each line corresponds to a single word, and the points on each line indicate the frequency and vector length for the tokens derived from the corresponding word.
For example, the five points on the line corresponding to \word{admiral} are labelled, from right to left, by the tokens \word{ADMIRAL\_1}, \word{ADMIRAL\_2}, \dots, \word{ADMIRAL\_5}.
The number of points on the line is determined by the frequency of the original word.
For example, the frequency of the \word{admiral} can be halved at most $5$ times and remain about the minimum frequency cut-off.

Because all the points on a line share a co-occurrence distribution (only the frequency varies), this figure demonstrates conclusively that length does indeed depend on frequency directly, and not only via some correlation between word frequency and semantic content.
Notice, moreover, that the relative positions of the norms of the word vectors associated to the test words is independent of the frequency band.
Therefore these lines trace lines along which a frequency-adjusted norm would be constant.


\begin{figure}
	\includegraphics[scale=0.5]{word-frequency-experiment-heatmap}
	\caption{
	Heatmap of the dot product of the normalised vectors associated to the
	tokens introduced into the corpus in the co-occurrence noise experiment
	(four such words chosen at random).  The largely red blocks demonstrate
	that the direction of the word vector does not change when noise is
	added to the co-occurrence distribution.
	}
	\label{fig:cooccurrence-noise-heatmap}
\end{figure}

\begin{sidewaysfigure*}\label{fig:word-frequency-experiment-graph}
	\centering{\includegraphics[scale=0.6]{word-frequency-experiment-graph}}
	\caption{
	Vector length vs. frequency for the words chosen for the word frequency
	experiment.  For each word, tokens of varying frequency but with the
	co-occurrence distribution of that word were introduced into the
	corpus, as described in section \ref{WFVE}.
	}
\end{sidewaysfigure*}

\end{section}

\begin{section}{Co-occurrence noise variation experiment}\label{CNVE}
Here, and throughout, the noise distribution is taken to be the global word frequency distribution.
Thus noise can be added to the cooccurrence distribution of a word by inspersing occurrences of that word uniformly at randomly throughout the corpus.
A small set of words is chosen from the unmodified corpus in the same manner as section \ref{WFVE}.

For each of the chosen words, the replacement procedure of subsection \ref{replacement-procedure} is performed using a geometric rate of decay of $\lambda = 5/6$, and truncating at $n=8$.
Subsequently, for every replacement token (e.g. \word{CAT\_i}) random occurrences of this token are interspersed uniformly at random throughout the corpus, such that the frequency of the replacement token is restored to that of the original token \word{cat}.
For example, if the original token \word{cat} occurred $1000$ times, then after the replacement procedure, \word{CAT\_2} occurs approximately $694$ times, so a further approximately $306 (=1000 - 694)$ random occurrences of \word{CAT\_2} are interspersed throughout the corpus.
Thus token \word{cat} is removed from the corpus, and a family of tokens \word{CAT\_i}, $1 \leqslant i \leqslant 8$ are introduced.
These tokens have the same frequency as \word{cat}, but their cooccurrence distributions, while based on that of \word{cat}, have an increasing amount of noise.
To be precise, the proportion of the occurrences of \word{CAT\_i} that arose by replacing \word{cat} is $P_{5/6}(i)$, and the proportion arising from its random intersperal throughout the corpus $(1 - P_{5/6}(i))$.

Figure \ref{fig:cooccurrence-noise-experiment-text} illustrates the effect of this modification, independently of the modifications of subsection \ref{WFVE}, in the case where the only word chosen is \word{cat}.
Note that for the experiments, \emph{both} modification procedures are performed, but with different sets of words.

\begin{figure}
	\input{noise-cooccurrence-experiment-counts.tex}
	\label{fig:cooccurrence-noise-counts}
	\caption{Occurrence counts for words chosen for the cooccurrence noise experiment. }
\end{figure}

% original text after modifications for the noise experiment with n=3 lambda 5/ 6 for cat
\begin{figure}
	\input{cooccurrence-noise-experiment.tex}
	\caption{Text modified for the co-occurrence noise experiment, where the word \word{cat} was chosen, $\lambda = 5/6$ and $n=3$.}
\label{fig:cooccurrence-noise-experiment-text}
\end{figure}

\begin{subsection}{Results}
Consider the effect on a word vector when increasing levels of noise added to the co-occurrence distribution of the word.
This is affected by randomly dispersing an increasing proporition $0 \leqslant p \leqslant 1$ of the pre-existing occurrences of that word throughout the corpus.
The word vector depends on $p$, and thus $p$ parameterises a path in the feature space.
This path begins (with $p=0$) at the original word vector, and ends (with $p=1$) at the vector associated with the meaningless vector \word{VOID}.

For each word (e.g. \word{jury}) in the co-occurrence noise experiment, the vectors associated with the tokens \word{JURY\_i} mark points along this word's path for increasing values of $p$.
Figure \ref{fig:cooccurrence-noise-heatmap} shows that these points at all co-linear.
Thus the path is a straight line.

Figure \ref{fig:cooccurrence-noise-graph} illustrates the relationship between the proportion $p$ of co-occurrence noise and vector length.
Remarkably, it is apparent that this relationship is linear for any individual word.
Thus the norm decreases at a constant velocity as $p$ travels from $0$ to $1$.

It is Figure \ref{fig:cooccurrence-noise-graph} that motivates the interpretation of vector length, within a sufficiently frequency band, as a measure of the absence of co-occurrence noise, that is, of the extent to which a word determines a context.

\begin{figure}
	\includegraphics[scale=0.5]{cooccurrence-noise-heatmap}
	\caption{
	Heatmap of the dot product of the normalised vectors associated to the
	tokens introduced into the corpus in the co-occurrence noise experiment
	(four such words chosen at random).  The largely red blocks demonstrate
	that the direction of the word vector does not change when noise is
	added to the co-occurrence distribution.
	}
	\label{fig:cooccurrence-noise-heatmap}
\end{figure}

\begin{sidewaysfigure*}\label{fig:cooccurrence-noise-graph}
	\includegraphics[scale=0.6]{cooccurrence-noise-graph}
	\caption{
	Vector length vs. proportion of co-occurrence noise for words
	chosen for the co-occurrence noise experiment.  For each word,
	tokens of equal frequency but with increasing proporitions of
	co-occurrence noise were introduced into the corpus, as
	described in section \ref{CNVE}.
	}
\end{sidewaysfigure*}

\end{subsection}

\end{section}


\section{Questions}
\begin{enumerate}
\item{ Why is the meaningless vector not at zero, and could the performance on the compositionality tasks be improved by accounting for this?  Notice that the meaningless vector is only updated as part of the context vector.  Would it be zero if we were performing skipgram? Does this have something to do with the lack of bias terms in the word2vec softmax?}
\item{Our arguments from the results subsection on the co-occurrence noise experiment indicate that the meaningless vector should indeed be taken as the centre of the space for word similarity tasks.  Check again whether this works experimentally.}
\item{What is the distribution of norms for the vectors relative to the meaningless vector?  Do I need to consider a different meaningless vector for each frequency band?  This is the measure that should represent meaning.}

\end{enumerate}


\begin{section}{offcuts}
Two controlled experiments are performed:
\begin{itemize}
	\item \emph{Word frequency variation} compare the lengths of the word vectors for words with varying frequencies, but with the same co-occurrence distribution;
	\item \emph{Co-occurrence noise variation} compare the lengths of the word vectors for words with of the same frequency, but with varying levels of noise in their co-occurrence distributions.
\end{itemize}
\end{section}

\footnotesize
\bibliographystyle{te}
\bibliography{main}
\end{document}
