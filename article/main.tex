% compile using latex (xelatex seems to crop the figure strangely)
\documentclass{article} % For LaTeX2e
\usepackage[dvips]{graphicx}
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{mdframed}
%\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[outdir=./]{epstopdf}

\usepackage{graphics,graphicx}
\usepackage{amsmath,amssymb,amsfonts,comment}
%\definecolor{LinkColor}{rgb}{0,0,0}    

\newcommand{\p}{\text{P}}
\newcommand{\word}[1]{\texttt{#1}}
\urldef{\googlegroup}\url{https://groups.google.com/forum/#!forum/word2vec-toolkit}
\urldef{\blogpost}\url{https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/}

\title{Corpus Experiments for Word Embeddings}

 \author{
 	Benjamin Wilson\\
	Lateral GmbH\\
	\texttt{benjamin@lateral.io}
	\And
	Adriaan Schakel\\
	\textcolor{red}{NNLP?}\\
	\texttt{adriaan.schakel@gmail.com}
 }

\date{\today}
\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\graphicspath{{../outputs/}}
\maketitle


\begin{abstract}
	We introduce an experimental approach to studying the properties of word embeddings.
	Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct, causal relationships between word properties and word vector direction and length.
	This approach is demonstrated in the case of the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise.
	In this case, it is shown that word vector length depends directly and linearly on both word frequency and the level of noise in the co-occurrence distribution of the word.
	In both cases, the coefficient of linearity depends upon the word.
\end{abstract} 

\begin{section}{Introduction}
Word embeddings are typically trained with samples from the word co-occurrence distributions.
Recall that the co-occurrence distribution of a word $w$ gives the probability $\p(w'|w)$ that a word $w'$ occurs nearby, given that $w$ occurred.
Samples of the co-occurrence distributions are obtained by scanning a short window over the text.
The neighbouring words are treated as samples of the co-occurrence distribution the current word. 
Viewed in this way, the word vectors of the word embedding are determined by the co-occurrence distributions and frequencies of the words in the corpus.

We perform experiments that illustrate the effect of these two determining pieces of information (the co-occurrence distribution and the word frequency) as they are varied independently of one another.
This is achieved through the introduction of new tokens into the training corpus with varying frequencies and varying levels of noise in their co-occurrence distributions.
The frequency and co-occurrence distributions of these introduced tokens are modeled on existing words in the corpus.

We illustrate our approach in case of the word2vec CBOW model.
However, these experiments (and other besides) could equally be carried out for word embedding methods such as word2vec skip-gram \cite{DistRepns,EfficientEstimation}, GloVe \cite{pennington2014glove} and SENNA \cite{collobert-2011}.

In the case of word2vec, we show that word vector length depends directly and linearly on both word-frequency and the level of noise in the co-occurrence distribution of the word.
In both cases, the coefficient of linearity depends upon the word.
If the co-occurrence distribution is fixed, then word vector length increases with word frequency.
On the other hand, if word frequency is held constant, then word vector length decreases as the level of noise in the co-occurrence distribution of the word is increased.

We show furthermore that word vector direction does not vary with word frequency or with the level of co-occurrence noise.

This paper is structured as follows.
Section \ref{related-work} draws connections to related work, while section \ref{corpus-and-model} describes the corpus and model used to illustrate the experiments.
Section \ref{WFVE} describes a controlled experiment for varying word frequency while holding the co-occurrence distribution fixed.
Section \ref{CNVE}, in a complementary fashion, describes a controlled experiment for varying the level of noise in the co-occurrence distribution of a word, while holding the word frequency fixed.
Section \ref{future-directions} considers further questions and possible future directions.
\end{section}

\begin{section}{Related work}\label{related-work}
Our experimental finding (in the context of word2vec), that word vector length decreases with co-occurrence noise is reminiscent of \cite{vecchi-baroni-zamparelli2011}, where a relationship between vector length and the ``semantic deviance'' of an adjective-noun composition was studied observationally.
In \cite{schakel-wilson}, the interpretation of word vector length as an indication of word significance is explored in the context of the arXiv high-energy physics corpus.

\textcolor{red}{what about some work introducing tokens into a corpus for experiments?}
\textcolor{red}{cite ``contextual distinctiveness: a new lexical property ...'' by McDonald and Shillcock?}

In the word2vec similarity and word relationship tasks, normalised word vectors are used.
Word vector length is thus disregarded.
In the word2vec forum\footnote{\googlegroup}, it has been observed that normalisation improves performance on these tasks since word vector length is related to word frequency.
It remained unclear, however, whether word frequency was \textit{directly} related to word vector length or rather via correlated factors such as word significance.
Our experimental approach demonstrates that a direct relationship indeed exists.
It is shown moreover that the normalisation of the word vectors also prevents word co-occurrence noise from affecting the performance of the word similarity and word relationship tasks.

On the other hand, recent theoretical work \cite{Arora2015} has approached the problem of explaining the so-called ``compositionality'' property exhibited by some word embeddings.
In that work, unnormalised vectors are used in their model of the word relationship task.
It is hoped that experimental approaches such as those described here might enable theoretical investigations to describe the role of the word vector length in the word relationship tasks.
\end{section}

\begin{section}{Corpus and model}\label{corpus-and-model}
Our training data is built from the Wikipedia datadump from October 2013.
In order to remove from the training data the bulk of robot-generated pages on Wikipedia, only pages with at least 20 monthly page views are retained\footnote{For further justification of this rationale, and to obtain the dataset, see \blogpost}.
Stubs and disambiguation pages are also removed, leaving 463 thousand pages with a total of 482 million words.
Word frequency is described in overview in Table \ref{word-occurrences-table}.

This base corpus is then modified as described in sections \ref{WFVE} and \ref{CNVE}.
Punctuation and numbers were removed and the corpus lower-cased prior to the modifications.
For recognisability, the tokens introduced into the corpus during the modification are uppercased.

For simplicity, only the word2vec CBOW word embedding with a single set of hyperparameters is considered.
Specifically, a $100$-dimensional CBOW model is trained using negative sampling with 5 negative samples, a window size of $10$, a minimum word occurrence of $128$, and $10$ passes through the corpus.
Sub-sampling was not used so that the influence of word frequency could be more clearly discerned.
Identical experimental results were obtained using hierarchical softmax, but these are omitted for succinctness.
The relatively high minimum word cut-off is chosen to ensure that word vectors, in all but degenerate cases, receive a sufficient number of gradient updates to be meaningful.
This cut-off results in a vocabulary of $81117$ words (only unigrams were considered).

The most recent revision of word2vec was used\footnote{SVN revision 42, see \url{http://word2vec.googlecode.com/svn/trunk/}}.
The source code for performing the experiments is made available on GitHub\footnote{\url{https://github.com/benjaminwilson/word2vec-norm-experiments}}.

Word vectors are then derived from the first layer of synaptic weights (``syn0'') and the second layer of weights (``syn1neg'') are discarded.

\begin{table}\label{word-occurrences-table}
	\input{words-occurrences.tex}
	\caption{ Number of words, by frequency band, as observed in the unmodified corpus.  }
\end{table}

\begin{subsection}{Replacement procedure}\label{replacement-procedure}
In the experiments, tokens are introduced into corpus via a replacement procedure.
To illustrate this procedure, consider a word, e.g. \word{cat}.
For each occurrence of this word, a sample $i$, $1 \leqslant i \leqslant n$ is drawn from a truncated geometric distribution, and that occurrence of \word{cat} is replaced with \word{CAT\_i}.
Thus the word \word{cat} is replaced throughout the corpus by a family of tokens with varying frequencies but approximately the same co-occurrence distribution as \word{cat}.

The geometric distribution is truncated in order to limit the number of tokens introduced into the corpus.
For any ratio $0 < \lambda < 1$ and maximum value $n > 0$, the truncated geometric distribution is given by
$$ P_{\lambda, n} (i) = \frac{\lambda^{i-1} \cdot (1-\lambda)}{(1 - \lambda^n)}, \qquad 1 \leqslant i \leqslant n.$$ 
This is the distribution over $i = 1, 2, \dots n$ for which the probabilities decay exponentially base $\lambda$.
Of course, other distributions might equally have been chosen for the experiments.
\end{subsection}
\end{section}

\begin{section}{Word frequency variation experiment}\label{WFVE}
In this experiment, families of tokens are introduced into the corpus.
The tokens in each family vary in frequency but share a common co-occurrence distribution.
After model training, comparison of the resulting word vectors associated to these tokens reveals the independent effect of frequency variation on the word embedding.

\begin{subsection}{Tokens derived from existing words}\label{WFVEexisting}
A small set of words from the unmodified corpus is chosen uniformly at random from the vocabulary.
In order that the introduced tokens do not have too low a frequency, only words which occur at least 10 thousand times are chosen.
To this set of words, the word \word{the} is added, in order to include a high-frequency stopword.
Table \ref{word-frequency-counts} lists the words chosen for this experiment along with their frequencies in the corpus.

The replacement procedure of subsection \ref{replacement-procedure} is then performed for each of these words, using a geometric rate of decay of $\lambda = 1/2$, and maximum value $n=20$.
This value of $\lambda$ is one of a range of values that ensure that, for each word, multiple tokens will be introduced with a number of occurrences sufficient to survive the chosen word2vec minimum occurrence cut-off of 128.  
A maximum value $n=20$ suffices for this choice of $\lambda$, since $2^{20 + \log_2{128}}$ exceeds the maximum occurrence of any token in the corpus. 
Figure \ref{fig:word-frequency-experiment-text-cat} illustrates the effect of these modifications on a sample text, with a family of tokens \word{CAT\_i}, derived from the word \word{cat}.
Notice that the word \word{cat} has been replaced throughout with the tokens \word{CAT\_i}.

\begin{table}\label{word-frequency-counts}
	\input{word-frequency-experiment-counts.tex}
	\caption{Occurrence counts for words chosen for the word frequency experiment. }
\end{table}

\begin{figure}\label{fig:word-frequency-experiment-text-cat}
	\begin{mdframed}
	\input{word-frequency-experiment-text-cat.tex}
	\end{mdframed}
	\caption{A text modified for the word frequency experiment as per subsection \ref{WFVEexisting}, where the
	word \word{cat} was chosen, $\lambda=0.5$ and $n=20$.}
\end{figure}

\end{subsection}

\begin{subsection}{Tokens derived from an introduced, meaningless word}\label{WFVEmeaningless}
	A high-frequency, purely meaningless word is included in the experiment for comparison.
	We choose to introduce a new, entirely meaningless token \word{VOID} into the corpus, rather than choose an existing word whose meaninglessness is only supposed.
	To achieve this, the token is interspersed uniformly at random throughout the corpus so that its global frequency is $f = 0.005$.
	The co-occurrence distribution of \word{VOID} thus coincides with the global frequency distribution.
	The replacement procedure is then performed for \word{VOID}, using the same values for $\lambda$ and $n$ as above.
	figure \ref{fig:word-frequency-experiment-text-void} shows the effect of this modifications on a sample text ($f = 0.05$ is used there for illustrative purposes).

\begin{figure}\label{fig:word-frequency-experiment-text-void}
	\begin{mdframed}
	\input{word-frequency-experiment-text-void.tex}
	\end{mdframed}
	\caption{A text modified for the word frequency experiment as per subsection \ref{WFVEmeaningless}, where $\lambda=0.5$ and $n=20$. For illustrative purposes, the token \word{VOID} was here interspersed with a global frequency of $0.05$.}
\end{figure}
\end{subsection}

\subsection{Results: word frequency and vector direction}\label{WFVE-direction}
Figure \ref{word-frequency-experiment-heatmap} shows the cosine similarity of the pairs of word vectors for the tokens introduced into the corpus.
Recall that the cosine similarity measures the extent to which two vectors have the same direction, taking a maximum value of $1$ and a minimum value of $-1$.
The number of word vectors associated to any word is the number of times that its frequency cat be halved and remain above the chosen minimum word count threshold of $128$.

Consider first the words other than \word{the} and \word{VOID}, e.g. \word{protestant}.
These words occur in a limited number of contexts, and so fewer samples are required to capture their co-occurrence distributions of such words.
Thus their word vectors can be learned with less samples.
It is therefore to be expected that the word vectors of their associated tokens can be properly learned from the modified corpus.
The red blocks of figure \ref{word-frequency-experiment-heatmap} clearly show that the direction of these word vectors depends only upon the original word.

The word \word{VOID}, on the other hand, occurs with probability in every context, by construction.
Thus a much higher number of samples are required to capture its co-occurrence distribution, and thereby to learn its word vector.
The same is true, but to a lesser extent, of the stopword \word{the}.
The ``difficulty'' of learning the word vector associated to \word{VOID\_i} or \word{the\_i} increases with $i$.
This is consistent with figure \ref{word-frequency-experiment-heatmap}, where it is apparent that the word vectors associated to \word{VOID\_i} for $i = 1, \dots, 5$ all share a common direction, while the word vectors for diverge directionallly for higher values of $i$.
The same trend is apparent, but less extremely so, for \word{the}.
The word vectors for \word{the\_i} share a common direction for $i = 1, \dots, 14$, before diverging directionally for higher values of $i$.
We conclude that these directional divergences are artifacts of poor learning due to an insufficient number of occurrences.
Table \ref{word-frequency-experiment-cosine-similarity}, showing the most similar words to the word vector of each \word{the\_i} provides further support for this claim.
In further discussion, the tokens \word{VOID\_i} (respectively, \word{the\_i}) whose word vectors have a cosine similarity less than $0.8$ with \word{VOID\_1} (respectively, \word{the\_1}) are therefore disregarded.

In conclusion, word frequency has no effect on word vector direction, given a sufficient number of samples to learn the co-occurrence distribution of the word.
%For well-learned representations, word frequency has no effect on word vector direction.

\subsection{Results: word frequency and vector length}
It remains, therefore, to consider the effect of frequency variation on word vector length.
Figure \ref{fig:word-frequency-experiment-graph} shows this relationship for individual words, both for the word vectors, ``syn0'' and for the discarded second-layer vectors ``syn1neg'' (included for completeness).
Each line corresponds to a single word, and the points on each line indicate the frequency and vector length for the tokens derived from the corresponding word.
For example, the six points on the line corresponding to \word{protestant} are labeled, from right to left, by the tokens \word{PROTESTANT\_1}, \word{PROTESTANT\_2}, \dots, \word{PROTESTANT\_6}.
Again, the number of points on the line is determined by the frequency of the original word.
For example, the frequency of the \word{protestant} can be halved at most $6$ times and remain about the minimum frequency cut-off.

Because all the points on a line share a co-occurrence distribution (only the frequency varies), figure \ref{fig:word-frequency-experiment-graph} demonstrates conclusively that length does indeed depend on frequency directly, and not only via some correlation between word frequency and word significance.
Moreover, this relationship is linear for any particular word.
Notice also that the relative positions of the norms of the word vectors associated to the experiment words is independent of the frequency band, i.e. the plotted lines rarely cross.
The plotted lines may thus be seen as the approximate contours of a hypothetical measure of word vector length normalised for word frequency effects.

Notice the inflection of the ``syn1neg'' curve for \word{the} at approximately $10^6$.  We have no explanation of this.

As an aside, notice also that the word vector length for the tokens \word{VOID\_i} is approximately constant.
Since the direction is also constant (cf. subsection \ref{WFVE-direction}), it is therefore sensible to speak of the word vector of \word{VOID} irrespective of its frequency.
In particular, \word{VOID\_1} may be taken as an approximation.

\begin{figure}\label{word-frequency-experiment-heatmap}
	\includegraphics[scale=0.5]{word-frequency-experiment-heatmap}
	\caption{
	Heatmap of the cosine similarity of the vectors associated to the
	tokens introduced into the corpus in the co-occurrence noise experiment
	(words other than \word{the} and \word{VOID} chosen randomly).
	}
\end{figure}

\begin{table}\label{word-frequency-experiment-cosine-similarity}
	\input{cosine-similarity.tex}
	\caption{
	The most similar words in the original vocabulary to the tokens
	introduced into the corpus associated to the word \word{the},
	and their cosine similarity with the most frequent such token,
	\word{THE\_1}.  It is apparent from the nearest neigbours that
	the word vectors of the low frequency tokens have not been
	adequately trained (there were inadequate samples).
	}
\end{table}

\begin{sidewaysfigure*}\label{fig:word-frequency-experiment-graph}
	\centering{\includegraphics[scale=0.6]{word-frequency-experiment-graph}}
	\caption{
	Vector length vs frequency for the words chosen for the word frequency
	experiment.  For each word, tokens of varying frequency but with the
	co-occurrence distribution of that word were introduced into the
	corpus, as described in section \ref{WFVE}.
	The word vectors are obtained from the first synaptic layer, ``syn0''.
	The second layer ``syn1neg'' is included for completeness.
	}
\end{sidewaysfigure*}

\end{section}

\begin{section}{Co-occurrence noise variation experiment}\label{CNVE}
Here, and throughout, the noise distribution is taken to be the global word frequency distribution.
Thus noise can be added to the co-occurrence distribution of a word by interspersing occurrences of that word uniformly at randomly throughout the corpus.
A small set of words is chosen from the unmodified corpus in the same manner as in section \ref{WFVE}.

For each of the chosen words, the replacement procedure of subsection \ref{replacement-procedure} is performed using a geometric rate of decay of $\lambda = 5/6$, and truncating at $n=8$.
Subsequently, for every replacement token (e.g. \word{CAT\_i}) random occurrences of this token are interspersed uniformly at random throughout the corpus, such that the frequency of the replacement token is restored to that of the original token \word{cat}.
For example, if the original token \word{cat} occurred $1000$ times, then after the replacement procedure, \word{CAT\_2} occurs approximately $694$ times, so a further (approximately) $306$ random occurrences of \word{CAT\_2} are interspersed throughout the corpus.
Thus token \word{cat} is removed from the corpus, and a family of tokens \word{CAT\_i}, $1 \leqslant i \leqslant 8$ are introduced.
These tokens all have the same frequency as \word{cat}, but their co-occurrence distributions, while based on that of \word{cat}, have an increasing amount of noise.

Figure \ref{fig:co-occurrence-noise-experiment-text} illustrates the effect of this modification, in the case where the only word chosen is \word{cat}.
The original text in this case concerned both cats and dogs.
Notice that the word \word{cat} has been replaced entirely in the cats section by occurrences of \word{CAT\_i} and moreover that occurrences of these same tokens appear also in the dogs section. These occurrences (and additionally, with probability, some occurrences from the cats section) are ``noise occurrences''.
Table \ref{cooccurrence-noise-words} lists the words chosen for the experiment, along with their frequencies in the corpus.

The proportion of the occurrences of \word{CAT\_i} that arose by replacing \word{cat} is $P_{5/6, 8}(i)$, and the proportion arising from the random interspersal of this token throughout the corpus is $r_i := (1 - P_{5/6, 8}(i))$.
The particular value of $\lambda$ (large compared to that of section \ref{WFVE}) was chosen so that the proportion of noise $r_i$ did not increase too quickly with $i$.
These proportions are the sampling points on our axis of variation, and this value of $\lambda$ (together with the choice of $n$) ensures a reasonable spread.
Note that other parameter values (or indeed other distributions) could equally have been used.

\begin{table}\label{cooccurrence-noise-words}
	\input{noise-cooccurrence-experiment-counts.tex}
	\label{fig:co-occurrence-noise-counts}
	\caption{Occurrence counts for words chosen for the co-occurrence noise experiment. }
\end{table}

\begin{figure}
	\begin{mdframed}
	\input{cooccurrence-noise-experiment.tex}
	\end{mdframed}
	\caption{A text modified for the co-occurrence noise experiment, where the word \word{cat} was chosen, $\lambda = 5/6$ and $n=3$.}
\label{fig:co-occurrence-noise-experiment-text}
\end{figure}

\begin{subsection}{Results}
	%The co-occurrence distributions of e.g. the tokens \word{}, for $i=1, \dots, 8$ increasingly approximate the global word frequency distribution, i.e. co-occurrence distribution of the token \word{VOID}.
	%Since the vectorisation of \word{VOID} is independent of its frequency (cf. remark \ref{}), the word vectors of \word{} can therefore be considered as a series of intermediate points on a hypothetical path in the feature space beginning at the word vector for \word{} and ending at the word vector for \word{VOID}.
	\textcolor{red}{all of the following is true (using the heatmap that i say it does) but it ignores the artifact that the path converges to the word vector for \word{VOID}, not to zero -- this is fine to the extent that they are approximately the same.}

Figure \ref{fig:co-occurrence-noise-heatmap} shows the cosine similarity of the pairs of word vectors for the tokens introduced into the corpus in this experiment.
Notice that the word vectors associated to tokens derived from the same word all share a common direction.
We conclude that co-occurrence noise has no effect on word vector direction.

On the other hand, figure \ref{fig:co-occurrence-noise-graph} shows that word vector length varies directly and linearly with the proportion $r$ of noise in the co-occurrence distribution.
This figure motivates an interpretation of vector length, within a sufficiently narrow frequency band, as a measure of the absence of co-occurrence noise, that is, of the extent to which a word determines a context.


\begin{figure}\label{fig:co-occurrence-noise-heatmap}
	\includegraphics[scale=0.5]{cooccurrence-noise-heatmap}
	\caption{
	Heatmap of the cosine similarity of some of the vectors associated to the
	tokens introduced into the corpus in the co-occurrence noise experiment
	(four such words chosen at random).  The
	largely red blocks demonstrate that the direction of the word vector
	does not change when noise is added to the co-occurrence distribution.
	}
\end{figure}

\begin{sidewaysfigure*}\label{fig:co-occurrence-noise-graph}
	\includegraphics[scale=0.6]{cooccurrence-noise-graph}
	\caption{
	Vector length vs proportion of co-occurrence noise for words
	chosen for the co-occurrence noise experiment.  For each word,
	tokens of equal frequency but with increasing proportions of
	co-occurrence noise were introduced into the corpus, as
	described in section \ref{CNVE}.
	The word vectors are obtained from the first synaptic layer, ``syn0''.
	The second layer ``syn1neg'' is included for completeness.
	}
\end{sidewaysfigure*}

\end{subsection}

\end{section}

\begin{section}{Discussion}\label{future-directions}

\begin{subsection}{Controlled experiments for word embeddings}
Our principle contribution has been to demonstrate that controlled experiments can be used to gain insight into a word embedding.
Because they are achieved via modification of the training corpus, these experiments can be carried out for any word embedding (or indeed language model), requiring no knowledge or modification of the model implementation.

It would, of course, be interesting to perform these experiments for other word embeddings other than word2vec CBOW and for different hyperparameters settings.

More elaborate experiments could be carried out.
For instance, by introducing tokens into the corpus that mix, with varying proportions, the co-occurrence distributions of two words, the path between the word vectors in the feature space could be studied.
The co-occurrence noise variation experiment described here would be a special case of such an experiment where one of the two words was \word{VOID}.
\end{subsection}

\begin{subsection}{Applications in other contexts}
Further afield, it would be interesting to perform analogs of these experiments in other contexts where models are trained from co-occurrence data.
For instance, in the context of e-Commerce, product purchase frequency and product co-occurrence noise can be varied by modifying the product-product co-occurrence matrix.
\end{subsection}

\begin{subsection}{Questions pertaining to word2vec}
Questions pertaining to word2vec in particular arise naturally from the results of the experiments.
Figures \ref{fig:frequency-norm-graph} and \ref{fig:co-occurrence-noise-graph}, for example, demonstrate the word vectors obtained from the first synaptic layer, ``syn0'', have very different properties from those that could be obtained from the second layer, ``syn1neg'', which are typically discarded.
These differences warrant further investigation.

The co-occurrence distribution of \word{VOID} is the global frequency distribution, and in this sense pure background noise.
Thus the word vector of \word{VOID} is a special point in the feature space.
Figure \ref{fig:frequency-norm-graph} shows that this point is not at the origin of the feature space (i.e. is not the zero vector).
The origin, however, is implicitly the point of reference in the word2vec word similarity task.
This raises the question of whether improved performance on the similarity task could be achieved by transforming the feature space (or modifying the model) such that the representation of pure noise (i.e. the vector for \word{VOID}) is at the origin of the transformed feature space. 

\textcolor{red}{can it be argued that the feature space must be fundamentally euclidean, given that the path marked by the vectorisations of intermediate words is a straight line? i.e. if such paths maybe considered as ``geodesics'' between words, and we show that these geodesics appear as straight lines when an euclidean metric is imposed, then suggests that the euclidean metric is in fact a good choice.  what would happen if we chose another metric?}

\end{subsection}

\end{section}

\clearpage
\footnotesize
\bibliography{main}
\bibliographystyle{plain}
\end{document}
